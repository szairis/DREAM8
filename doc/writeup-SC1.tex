\documentclass[letterpaper, 11pt]{article}
\usepackage{helvet}
\usepackage[margin=0.75in]{geometry}
\usepackage{datetime}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{authblk}
\usepackage{enumerate}
\usepackage[colorlinks=true]{hyperref}
\pagestyle{plain}
\pagenumbering{arabic}
\bibliographystyle{plain}

\title{Network Inference\\Using Gradient Boosted Decision Trees}

\author[1]{Kevin Emmett\thanks{kje2109@columbia.edu}}
\author[2]{Sakellarios Zairis\thanks{siz2102@columbia.edu}}
\affil[1]{Department of Physics, Columbia University}
\affil[2]{Department of Computational Biology \& Bioinformatics, Columbia University}

\setcounter{section}{-1}

\begin{document}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary}

We first train gradient boosting regression trees on the time series data and then derive the network connectivity from the most frequently selected features over the boosting rounds.  We also incorporate a certain degree of prior biological knowledge into the adjacency matrix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}

\subsection{Background}

The general gradient boosting tree algorithm is as follows \citep{Hastie2004}:

\begin{enumerate}
\item Initialize $f_{0}(x) = \arg\min_{\gamma}\sum_{i=1}^N L(y_{i}, \gamma)$
\item For $m=1$ to $M$:
  \begin{enumerate}[(a)]
    \item For $i=1,2,\cdots,N$ compute
      \begin{equation}
        r_{im} = -\left[\frac{\partial L(y_{i},f(x_{i}))}{\partial f(x_{i})}\right]_{f=f_{m-1}}
      \end{equation}
    \item Fit a regression tree to the targets $r_{im}$ giving terminal regions $R_{jm}$, $j=1,2,\cdots,J_m$.
    \item For $j=1,2,\cdots,J_{m}$ compute
      \begin{equation}
        \gamma_{jm} = \arg\min_{\gamma}\displaystyle\sum_{x_{i}\in R_{jm}} L(y_{i}, f_{m-1}(x_{i})+\gamma)
      \end{equation}
    \item Update $f_{m}(x) = f_{m-1}(x) + \sum_{j=1}^{J_m} \gamma_{jm} I(x\in R_{jm})$.
  \end{enumerate}
\item Output $\hat{f}(x) = f_{M}(x)$
\end{enumerate}

The salient features of this algorithm are (a) it builds a strong classifier from an ensemble of weaker ones, (b) it re-weights the training data at each iteration to emphasize hitherto incorrectly classified examples, and (c) the final classifier provides not only the $sign$ but also its margin, or distance from zero, as a measure of confidence in the prediction.

All feature weights are rescaled to the interval [0, 1] and are used to populate the adjacency matrix. In this approach, therefore, if antibody X was repeatedly used to build the predictive function for antibody Y, then we assume that X is a parent of Y in the network.

\begin{figure}
  \centering
  \includegraphics[width=1.0\textwidth]{deviance_feature_importance.png}
  \caption{Training the GBR model for insilico dataset, AB15. Left: Training set deviance for GBR model as function of boosting round. Right: Feature importance rankings.}
\end{figure}
\subsection{Implementation}

Our models were written in python. We used \href{http://pandas.pydata.org/}{pandas} to manipulate the data prior to training, and implemented our models using \href{http://scikit-learn.org/stable/}{scikit-learn}.

\subsection{Parameter Tuning}

Gradient Boosted Decision trees have the following parameters:
\begin{itemize}
  \item \texttt{num\_estimators}: number of rounds of boosting rounds to perform.
  \item \texttt{max\_depth}: maximum depth of the base learners.
  \item \texttt{learning\_rate}: the learning rate shrinks the contribution of each tree by the specified value.
\end{itemize}

We used cross validation to set the best values of the parameters for each trained model.

\subsection{Inhibitor/Stimulus Modeling}

Inhibitors were modeled using a perfect fixed-effects model \citep{Spencer2012}. The stimulus was not explicitly modeled. We dealt with stimulii in two ways: by grouping datasets across stimulii, and by training independent models for each stimulus. We found training separate models for each stimulus performed better.

\subsection{Experimental vs inSilico}

For subchallenge 1, we used the same general approach in both 1A and 1B. The only key difference is our use of a curated network prior in 1A. The prior is discussed further in the External Information section below.

\subsection{Any other approaches considered?}

Our first approach to modeling time series were based on symbolic regression using the genetic algorithm package \href{http://creativemachines.cornell.edu/eureqa}{Eureqa}. While this approach was promising, it was computationally expensive. We also implemented a simple Dynamic Bayesian Network, following \citep{Hill2012}. We found the DBN overfit when used to predict time courses. Before settling on the GBR model, we tried a Lasso regularized linear model, which we found performed slightly worse under cross validation.

\subsection{Computational Resources}

Our algorithms ran on our local machines (Intel Core i7, 8GB RAM), typically taking no more than 5 minutes per run.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Preparation}

For the experimental data, we partitioned the data into independent subsets for each cell line and stimulus combination (\texttt{cell\_line}, \texttt{stim}). We modeled inhibitors using a perfect fixed-effects model \citep{Spencer2012}. We used only the ``Main'' dataset in training our models.

For the insilico data, the dataset was partitioned into independent subsets for each stimulus. Inhibitors were again modeled using a perfect fixed-effects model.

For each dataset, we centered and scaled the columns before training the model. We did not use log-transformed data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{External Information}

We implemented two literature models of dynamic signaling networks: a 16 node ERK pathway \citep{Xu2010} and a 5 node yeast network \citep{Cantone2009}. We used these as gold standard networks to prototype our model. In these datasets, we had a known network structure which  we used to simulate synthetic time courses.

A thorough literature review for canonical cancer signaling mechanisms was also performed.  This yielded a set of network edges which the authors bias toward in the inferred adjacency matrix for 1A.  This prior represents the basic understanding of RTK signaling pathways.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Validation}

We used k-fold cross validation to test our model performance (k=5, k=10, LOO). We did not model randomness in our cross-validation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Leader Board}

We found the leader board had limited utility for subchallenge 1 because of the lack of a clearly defined scoring function. The main use we had for the leader board was in calibrating our network prior's weighting.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

We implemented code to use the gradient of the concentration as the output variable, but didn't get a chance to fully test it. Additionally, we plan to continue experimenting with different post-processing logic for combining adjacency matrices derived from different stimulus/inhibitor conditions. We are pleased nonetheless at our fairly successful application of a powerful method like gradient tree boosting to the domain of network inference where it appears under-utilized.  We anticipate strong performance of this approach as the number of phosphoproteins reported on RPPA chips increases and the danger of overfitting in other methods increases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Author Backgrounds and Statements}

Kevin Emmett is a PhD student in physics with a background in discriminative modeling of emerging pathogens. Sakellarios Zairis is an MD/PhD student with a background in applying supervised learning techniques to predicting viral oncogenic potential.  These authors contributed equally to the work, with an average weekly time commitment of 15 hours.

\bibliography{DREAM8}

\end{document}
